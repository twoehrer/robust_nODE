{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from plots.gifs import trajectory_gif\n",
    "from plots.plots import get_feature_history, plt_train_error, plt_norm_state, plt_norm_control, plt_classifier, feature_plot, plt_dataset\n",
    "from models.training import Trainer, robTrainer\n",
    "from models.neural_odes import NeuralODE, robNeuralODE\n",
    "from models.resnets import ResNet\n",
    "import pickle\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#if trainin false, models will be loaded from file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim, data_dim = 2, 2\n",
    "T, num_steps = 5.0, 10  #T is the end time, num_steps are the amount of discretization steps for the ODE solver\n",
    "dt = T/num_steps\n",
    "turnpike = True\n",
    "bound = 0.\n",
    "fp = False\n",
    "cross_entropy = True\n",
    "noise = 0.15\n",
    "\n",
    "\n",
    "training = True #train new network or load saved one\n",
    "num_epochs = 5 #number of optimization epochs for gradient decent\n",
    "\n",
    "if turnpike:\n",
    "    weight_decay = 0 if bound>0. else dt*0.01\n",
    "else: \n",
    "    weight_decay = dt*0.01          #0.01 for fp, 0.1 else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "anode = NeuralODE(device, data_dim, hidden_dim, augment_dim=0, non_linearity='tanh', \n",
    "                    architecture='outside', T=T, time_steps=num_steps, fixed_projector=fp, cross_entropy=cross_entropy)\n",
    "\n",
    "optimizer_anode = torch.optim.Adam(anode.parameters(), lr=1e-3, weight_decay=weight_decay) #weight decay parameter modifies norm\n",
    "trainer_anode = Trainer(anode, optimizer_anode, device, cross_entropy=cross_entropy, \n",
    "                        turnpike=turnpike, bound=bound, fixed_projector=fp, verbose = False)\n",
    "\n",
    "\n",
    "anode_test = NeuralODE(device, data_dim, hidden_dim, augment_dim=0, non_linearity='tanh', \n",
    "                    architecture='outside', T=T, time_steps=num_steps, fixed_projector=fp, cross_entropy=cross_entropy)\n",
    "\n",
    "optimizer_anode_test = torch.optim.Adam(anode_test.parameters(), lr=1e-3, weight_decay=weight_decay) #weight decay parameter modifies norm\n",
    "trainer_anode_test = Trainer(anode_test, optimizer_anode_test, device, cross_entropy=cross_entropy, \n",
    "                        turnpike=turnpike, bound=bound, fixed_projector=fp, verbose = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rob_node = robNeuralODE(device, data_dim, hidden_dim, augment_dim=0, non_linearity='tanh', \n",
    "                            architecture='outside', T=T, time_steps=num_steps, fixed_projector=fp, cross_entropy=cross_entropy)\n",
    "\n",
    "\n",
    "optimizer_rob_node = torch.optim.Adam(rob_node.parameters(), lr=1e-3, weight_decay=weight_decay) #weight decay parameter modifies norm\n",
    "trainer_rob_node = robTrainer(rob_node, optimizer_rob_node, device, cross_entropy=cross_entropy, \n",
    "                        turnpike=turnpike, bound=bound, fixed_projector=fp, verbose = False)\n",
    "\n",
    "\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initializing self.adjoint_flow creates additional layers into rob_node. Does this effect the learning? Or can I just save/load only the important ones and it will be fine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Tobi/FAUbox/Python/borjan dynamical.systems/models/training.py:116: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softpred = m(y_pred)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_t torch.Size([2, 2]) b_t torch.Size([2]) x torch.Size([64, 2]) x_traj torch.Size([10, 64, 2]) p torch.Size([64, 2])\n",
      "w_t torch.Size([2, 2]) b_t torch.Size([2]) x torch.Size([64, 2]) x_traj torch.Size([10, 64, 2]) p torch.Size([64, 2])\n",
      "w_t torch.Size([2, 2]) b_t torch.Size([2]) x torch.Size([64, 2]) x_traj torch.Size([10, 64, 2]) p torch.Size([64, 2])\n",
      "w_t torch.Size([2, 2]) b_t torch.Size([2]) x torch.Size([64, 2]) x_traj torch.Size([10, 64, 2]) p torch.Size([64, 2])\n",
      "w_t torch.Size([2, 2]) b_t torch.Size([2]) x torch.Size([64, 2]) x_traj torch.Size([10, 64, 2]) p torch.Size([64, 2])\n",
      "w_t torch.Size([2, 2]) b_t torch.Size([2]) x torch.Size([64, 2]) x_traj torch.Size([10, 64, 2]) p torch.Size([64, 2])\n",
      "w_t torch.Size([2, 2]) b_t torch.Size([2]) x torch.Size([64, 2]) x_traj torch.Size([10, 64, 2]) p torch.Size([64, 2])\n",
      "w_t torch.Size([2, 2]) b_t torch.Size([2]) x torch.Size([64, 2]) x_traj torch.Size([10, 64, 2]) p torch.Size([64, 2])\n",
      "w_t torch.Size([2, 2]) b_t torch.Size([2]) x torch.Size([64, 2]) x_traj torch.Size([10, 64, 2]) p torch.Size([64, 2])\n",
      "w_t torch.Size([2, 2]) b_t torch.Size([2]) x torch.Size([64, 2]) x_traj torch.Size([10, 64, 2]) p torch.Size([64, 2])\n",
      "w_t torch.Size([2, 2]) b_t torch.Size([2]) x torch.Size([64, 2]) x_traj torch.Size([10, 64, 2]) p torch.Size([64, 2])\n",
      "w_t torch.Size([2, 2]) b_t torch.Size([2]) x torch.Size([64, 2]) x_traj torch.Size([10, 64, 2]) p torch.Size([64, 2])\n",
      "w_t torch.Size([2, 2]) b_t torch.Size([2]) x torch.Size([64, 2]) x_traj torch.Size([10, 64, 2]) p torch.Size([64, 2])\n",
      "w_t torch.Size([2, 2]) b_t torch.Size([2]) x torch.Size([64, 2]) x_traj torch.Size([10, 64, 2]) p torch.Size([64, 2])\n",
      "w_t torch.Size([2, 2]) b_t torch.Size([2]) x torch.Size([64, 2]) x_traj torch.Size([10, 64, 2]) p torch.Size([64, 2])\n",
      "w_t torch.Size([2, 2]) b_t torch.Size([2]) x torch.Size([64, 2]) x_traj torch.Size([10, 64, 2]) p torch.Size([64, 2])\n",
      "w_t torch.Size([2, 2]) b_t torch.Size([2]) x torch.Size([64, 2]) x_traj torch.Size([10, 64, 2]) p torch.Size([64, 2])\n",
      "w_t torch.Size([2, 2]) b_t torch.Size([2]) x torch.Size([64, 2]) x_traj torch.Size([10, 64, 2]) p torch.Size([64, 2])\n",
      "w_t torch.Size([2, 2]) b_t torch.Size([2]) x torch.Size([64, 2]) x_traj torch.Size([10, 64, 2]) p torch.Size([64, 2])\n",
      "w_t torch.Size([2, 2]) b_t torch.Size([2]) x torch.Size([64, 2]) x_traj torch.Size([10, 64, 2]) p torch.Size([64, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x2 and 64x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mr/rhl90_4n1sl5ykvnqh3d18t40000gn/T/ipykernel_28964/1833022333.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m#trainer_anode_test.train(dataloader, num_epochs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mtrainer_rob_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'anode.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/FAUbox/Python/borjan dynamical.systems/models/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_loader, num_epochs)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch {}: {:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/FAUbox/Python/borjan dynamical.systems/models/training.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_loader, epoch)\u001b[0m\n\u001b[1;32m    255\u001b[0m                         \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                         loss = beta*sum([self.loss_func(traj[k], y_batch)+self.loss_func(traj[k+1], y_batch) \n\u001b[0;32m--> 257\u001b[0;31m                                         for k in range(time_steps-1)]) + rob_factor*adj_traj[-1].matmul(adj_traj[-1])\n\u001b[0m\u001b[1;32m    258\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x2 and 64x2)"
     ]
    }
   ],
   "source": [
    "'''Train or load the models'''\n",
    "\n",
    "#%%capture #this surpresses output\n",
    "shuffle = False\n",
    "\n",
    "\n",
    "#old source for data points\n",
    "#with open('data.txt', 'rb') as fp:\n",
    " #   data_line, test = pickle.load(fp)\n",
    "\n",
    "\n",
    "\n",
    "X, y = make_circles(3000, noise=noise, factor=0.15, random_state=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.05)\n",
    "\n",
    "X_train = torch.Tensor(X_train) # transform to torch tensor for dataloader\n",
    "y_train = torch.Tensor(y_train) #transform to torch tensor for dataloader\n",
    "\n",
    "X_test = torch.Tensor(X_train) # transform to torch tensor for dataloader\n",
    "y_test = torch.Tensor(y_train) #transform to torch tensor for dataloader\n",
    "\n",
    "X_train = X_train.type(torch.float32)  #type of orginial pickle.load data\n",
    "y_train = y_train.type(torch.int64) #dtype of original picle.load data\n",
    "\n",
    "X_test = X_test.type(torch.float32)  #type of orginial pickle.load data\n",
    "y_test = y_test.type(torch.int64) #dtype of original picle.load data\n",
    "\n",
    "\n",
    "data_line = TensorDataset(X_train,y_train) # create your datset\n",
    "test = TensorDataset(X_test, y_test)\n",
    "\n",
    "dataloader = DataLoader(data_line, batch_size=64, shuffle=shuffle)\n",
    "dataloader_viz = DataLoader(data_line, batch_size=128, shuffle=shuffle)\n",
    "\n",
    "\n",
    "if training:\n",
    "    trainer_anode.train(dataloader, num_epochs)\n",
    "    #trainer_anode_test.train(dataloader, num_epochs)\n",
    "\n",
    "    trainer_rob_node.train(dataloader, num_epochs)\n",
    "\n",
    "    torch.save(anode.state_dict(), 'anode.pth')\n",
    "    torch.save(rob_node.state_dict(), 'rob_node.pth')\n",
    "else:\n",
    "    anode.load_state_dict(torch.load('anode.pth'))\n",
    "    rob_node.load_state_dict(torch.load('rob_node.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rob_node)\n",
    "# print(anode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_base = '1traj'\n",
    "\n",
    "filename_s = filename_base + '_s'\n",
    "filename_r = filename_base + '_r'\n",
    "\n",
    "print('normal')\n",
    "plt_classifier(anode, data_line, test, num_steps=10, save_fig = '1generalization.png') \n",
    "\n",
    "print('robust')\n",
    "plt_classifier(rob_node, data_line, test, num_steps=10, save_fig = '1rob_generalization.png') \n",
    "\n",
    "\n",
    "# plt.figure(1)\n",
    "# plt.subplot(121)\n",
    "# plt.imshow(imageio.imread('1generalization.png'))\n",
    "# plt.title('standard')\n",
    "# plt.axis('off')\n",
    "# plt.subplot(122)\n",
    "# plt.imshow(imageio.imread(filename_r + '29.png'))\n",
    "# plt.title('augmented robustness')\n",
    "# plt.axis('off')\n",
    "\n",
    "inputs, targets = next(iter(dataloader_viz))\n",
    "print(sum(inputs))\n",
    "\n",
    "\n",
    "\n",
    "trajectory_gif(anode, inputs, targets, timesteps=num_steps, filename = filename_s +'.gif')\n",
    "trajectory_gif(rob_node, inputs, targets, timesteps=num_steps, filename = filename_r + '.gif')\n",
    "filename_base = '1traj'\n",
    "\n",
    "\n",
    "# plt.figure(1)\n",
    "# plt.subplot(121)\n",
    "# plt.imshow(imageio.imread(filename_s + '29.png'))\n",
    "# plt.title('standard')\n",
    "# plt.axis('off')\n",
    "# plt.subplot(122)\n",
    "# plt.imshow(imageio.imread(filename_r + '29.png'))\n",
    "# plt.title('augmented robustness')\n",
    "# plt.axis('off')\n",
    "\n",
    "# plt.savefig('1comparison_' + filename_base + '.png',\n",
    "#                     format='png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# from IPython.display import Image\n",
    "# Image(filename='1comparison_1traj.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rob_node.state_dict)\n",
    "\n",
    "print('anode')\n",
    "\n",
    "for name, param in anode.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "\n",
    "\n",
    "print('\\n','anode_test')\n",
    "\n",
    "\n",
    "for name, param in anode_test.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "\n",
    "        \n",
    "        \n",
    "print('\\n','rob_node')\n",
    "\n",
    "\n",
    "for name, param in rob_node.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =torch.tensor([[1,2],[3,4]])\n",
    "print(x)\n",
    "print(torch.mm(x,x+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w1_t = anode.state_dict.fc1_time[1].weight\n",
    "wt = anode.flow.dynamics.fc2_time[0].weight\n",
    "bt = anode.flow.dynamics.fc2_time[0].bias\n",
    "\n",
    "x = torch.zeros(100,2)\n",
    "x[0] = torch.tensor([1.,2.])\n",
    "w = torch.tensor([[0.,-1.],[1.,0.]])\n",
    "print('x', x[0])\n",
    "print('inner',torch.matmul(w,x[0])+ x[0]) \n",
    "inner = torch.matmul(w,x[0])+ x[0]\n",
    "inner = torch.diag_embed(inner)\n",
    "print('embedd', torch.diag_embed(inner))\n",
    "outer = torch.matmul(w.t(),inner)\n",
    "print('outer',outer)\n",
    "print('final', torch.matmul(outer,x[0]))\n",
    "# print(torch.matmul(mat.t(),x[1]))\n",
    "\n",
    "out = torch.matmul(x,w.t()) + x\n",
    "out = torch.diag_embed(out)\n",
    "out = torch.matmul(w.t(),out)\n",
    "x = x[:,:,None]\n",
    "out = torch.matmul(out,x)\n",
    "# print(out)\n",
    "print('x',x.size(),'out', out.size())\n",
    "print(out.size())\n",
    "print(out.flatten(start_dim=1).size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn(10,2)\n",
    "print(torch.diag_embed(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1954, 0.6564])\n",
      "tensor([[1.1954],\n",
      "        [0.6564]])\n",
      "torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2)\n",
    "print(x)\n",
    "x = x.unsqueeze(-1)\n",
    "print(x)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.2445,  0.2335])\n",
      "torch.Size([128, 2])\n",
      "tensor([-2.2445,  0.2335])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.1373)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = torch.tensor([1.,0.])\n",
    "x = torch.randn(64,2)\n",
    "# print(torch.zeros(x.size()) + torch.tensor([1,0]))\n",
    "# torch.matmul(,p1)\n",
    "print(x[0])\n",
    "test = torch.cat((x, x))\n",
    "print(test.size())\n",
    "print(x[0])\n",
    "torch.matmul(test[-1],test[-1])\n",
    "\n",
    "\n",
    "#I NEED TO MAKE THE TRAJECTOREIS OF THE ADJOINT ADD UP CORRECTLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c0c5483d19f4fe5b21c2c2570e90b3190ed247dc8e6ba08ec1bd3c7fe8667884"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}